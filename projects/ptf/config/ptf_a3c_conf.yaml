N_WORKERS: 1
USE_CPU_COUNT: True
option_model_path: ['1','2','3']
memory_size: 2000000
reward_decay: 0.99
e_greedy: 0.9
replace_target_iter: 1000
batch_size: 32
option_batch_size: 32
e_greedy_increment: 0.0005
start_greedy: 0.0
output_graph: True
save_model: True
learning_step: 1000
summary_output_times: 1
regular: 0.005
learning_rate_a: 0.0001
learning_rate_c: 0.0001
learning_rate_o: 0.0005
learning_rate_t: 0.0005
ENTROPY_BETA: 0.01
source_policy: 'dqn'
load_model: False
load_model_path: ''
c1: 0.001
c3: 0.0005
clip_value: 10.0
xi: 0
learning_times: 1

#run
reward_memory: 100
save_per_episodes: 2000

# network
GLOBAL_NET_SCOPE: 'Global_Net'
option_layer_1: 20
n_layer_a_1: 64
n_layer_a_2: 64
n_layer_c_1: 64
n_layer_c_2: 64

# output
SAVE_PATH: "model"
graph_path: "graph"
reward_output: "output"
output_filename: "out"
log: "log"